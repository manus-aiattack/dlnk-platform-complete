"""
Exploit Validator and Tester
Tests exploits for reliability and effectiveness
"""

import asyncio
from core.base_agent import BaseAgent
from core.data_models import AgentData, Strategy
import subprocess
import tempfile
from typing import Dict, List, Optional
from pathlib import Path
import logging

log = logging.getLogger(__name__)


class ExploitTester:
    """
    Exploit Validator and Tester
    
    Features:
    - Test exploit reliability
    - Multi-target testing
    - Success rate calculation
    - Exploit scoring
    """
    
    def __init__(self):
        self.test_results = []
        self.success_count = 0
        self.failure_count = 0
    
    async def test_exploit(
        self,
        exploit_code: str,
        target_url: str,
        iterations: int = 10
    ) -> Dict:
        """
        Test exploit multiple times to determine reliability
        
        Args:
            exploit_code: Exploit code/payload
            target_url: Target URL
            iterations: Number of test iterations
        
        Returns:
            Test results with success rate
        """
        log.info(f"[ExploitTester] Testing exploit against {target_url} ({iterations} iterations)")
        
        results = {
            'target': target_url,
            'iterations': iterations,
            'successes': 0,
            'failures': 0,
            'timeouts': 0,
            'errors': [],
            'success_rate': 0.0,
            'avg_response_time': 0.0,
            'reliability_score': 0.0
        }
        
        response_times = []
        
        for i in range(iterations):
            try:
                test_result = await self._execute_exploit_test(
                    exploit_code, target_url, iteration=i+1
                )
                
                if test_result['success']:
                    results['successes'] += 1
                    self.success_count += 1
                elif test_result.get('timeout', False):
                    results['timeouts'] += 1
                else:
                    results['failures'] += 1
                    self.failure_count += 1
                
                if test_result.get('response_time'):
                    response_times.append(test_result['response_time'])
                
                if test_result.get('error'):
                    results['errors'].append(test_result['error'])
                
            except Exception as e:
                log.error(f"[ExploitTester] Test iteration {i+1} failed: {e}")
                results['failures'] += 1
                results['errors'].append(str(e))
        
        # Calculate metrics
        results['success_rate'] = results['successes'] / iterations
        results['avg_response_time'] = sum(response_times) / len(response_times) if response_times else 0.0
        results['reliability_score'] = self._calculate_reliability_score(results)
        
        log.info(f"[ExploitTester] Test complete: {results['success_rate']:.1%} success rate, "
                f"reliability: {results['reliability_score']:.2f}")
        
        self.test_results.append(results)
        
        return results
    
    async def _execute_exploit_test(
        self,
        exploit_code: str,
        target_url: str,
        iteration: int
    ) -> Dict:
        """Execute single exploit test"""
        
        import time
        start_time = time.time()
        
        # Real exploit execution
        log.debug(f"[ExploitTester] Iteration {iteration}: Executing exploit...")
        
        try:
            # Create temporary file for exploit
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(exploit_code)
                exploit_file = f.name
            
            # Execute exploit with timeout
            process = await asyncio.create_subprocess_exec(
                'python3', exploit_file, target_url,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=30.0
                )
                
                response_time = time.time() - start_time
                
                # Check if exploit succeeded
                success = process.returncode == 0
                error = stderr.decode() if stderr else None
                
                # Clean up
                Path(exploit_file).unlink(missing_ok=True)
                
                return {
                    'success': success,
                    'response_time': response_time,
                    'timeout': False,
                    'error': error,
                    'stdout': stdout.decode() if stdout else None
                }
                
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                Path(exploit_file).unlink(missing_ok=True)
                
                return {
                    'success': False,
                    'response_time': time.time() - start_time,
                    'timeout': True,
                    'error': 'Exploit execution timeout'
                }
                
        except Exception as e:
            log.error(f"[ExploitTester] Exploit execution failed: {e}")
            return {
                'success': False,
                'response_time': time.time() - start_time,
                'timeout': False,
                'error': str(e)
            }
    
    async def execute(self, strategy: Strategy) -> AgentData:
        """Execute attack"""
        try:
            target = strategy.context.get('target_url', '')
            
            # Implement attack logic here
            results = {'status': 'not_implemented'}
            
            return AgentData(
                agent_name=self.__class__.__name__,
                success=True,
                summary=f"{self.__class__.__name__} executed",
                errors=[],
                execution_time=0,
                memory_usage=0,
                cpu_usage=0,
                context={'results': results}
            )
        except Exception as e:
            return AgentData(
                agent_name=self.__class__.__name__,
                success=False,
                summary=f"{self.__class__.__name__} failed",
                errors=[str(e)],
                execution_time=0,
                memory_usage=0,
                cpu_usage=0,
                context={}
            )

    def _calculate_reliability_score(self, results: Dict) -> float:
        """Calculate reliability score (0.0 - 1.0)"""
        
        score = 0.0
        
        # Success rate (60%)
        score += results['success_rate'] * 0.6
        
        # Consistency (20%) - penalize timeouts
        consistency = 1.0 - (results['timeouts'] / results['iterations'])
        score += consistency * 0.2
        
        # Speed (20%) - faster is better
        if results['avg_response_time'] > 0:
            speed_score = min(1.0, 1.0 / results['avg_response_time'])
            score += speed_score * 0.2
        
        return score
    
    async def test_multi_target(
        self,
        exploit_code: str,
        targets: List[str],
        iterations_per_target: int = 5
    ) -> Dict:
        """
        Test exploit against multiple targets
        
        Args:
            exploit_code: Exploit code/payload
            targets: List of target URLs
            iterations_per_target: Test iterations per target
        
        Returns:
            Multi-target test results
        """
        log.info(f"[ExploitTester] Testing exploit against {len(targets)} targets")
        
        results = {
            'total_targets': len(targets),
            'successful_targets': 0,
            'failed_targets': 0,
            'target_results': [],
            'overall_success_rate': 0.0
        }
        
        for target in targets:
            target_result = await self.test_exploit(
                exploit_code, target, iterations_per_target
            )
            
            results['target_results'].append(target_result)
            
            if target_result['success_rate'] >= 0.5:
                results['successful_targets'] += 1
            else:
                results['failed_targets'] += 1
        
        # Calculate overall success rate
        all_success_rates = [r['success_rate'] for r in results['target_results']]
        results['overall_success_rate'] = sum(all_success_rates) / len(all_success_rates)
        
        log.info(f"[ExploitTester] Multi-target test complete: "
                f"{results['successful_targets']}/{results['total_targets']} targets exploitable")
        
        return results
    
    async def validate_exploit_requirements(
        self,
        exploit_code: str,
        requirements: Dict
    ) -> Dict:
        """
        Validate that exploit meets requirements
        
        Args:
            exploit_code: Exploit code
            requirements: Required capabilities
        
        Returns:
            Validation results
        """
        log.info("[ExploitTester] Validating exploit requirements...")
        
        validation = {
            'valid': True,
            'issues': [],
            'warnings': []
        }
        
        # Check size requirements
        if 'max_size' in requirements:
            if len(exploit_code) > requirements['max_size']:
                validation['valid'] = False
                validation['issues'].append(
                    f"Exploit size ({len(exploit_code)}) exceeds maximum ({requirements['max_size']})"
                )
        
        # Check bad characters
        if 'bad_chars' in requirements:
            for bad_char in requirements['bad_chars']:
                if bad_char in exploit_code:
                    validation['valid'] = False
                    validation['issues'].append(f"Contains bad character: {bad_char}")
        
        # Check required capabilities
        if 'capabilities' in requirements:
            for capability in requirements['capabilities']:
                if not self._check_capability(exploit_code, capability):
                    validation['warnings'].append(f"May not support capability: {capability}")
        
        return validation
    
    def _check_capability(self, exploit_code: str, capability: str) -> bool:
        """Check if exploit supports capability"""
        
        # Simplified capability checking
        capability_keywords = {
            'remote_code_execution': ['exec', 'system', 'shell'],
            'privilege_escalation': ['sudo', 'setuid', 'root'],
            'persistence': ['cron', 'service', 'startup'],
            'data_exfiltration': ['curl', 'wget', 'nc', 'netcat']
        }
        
        keywords = capability_keywords.get(capability, [])
        return any(keyword in exploit_code.lower() for keyword in keywords)
    
    async def benchmark_exploit(
        self,
        exploit_code: str,
        target_url: str,
        duration: int = 60
    ) -> Dict:
        """
        Benchmark exploit performance
        
        Args:
            exploit_code: Exploit code
            target_url: Target URL
            duration: Benchmark duration in seconds
        
        Returns:
            Benchmark results
        """
        log.info(f"[ExploitTester] Benchmarking exploit for {duration}s...")
        
        import time
        start_time = time.time()
        
        benchmark = {
            'duration': duration,
            'total_attempts': 0,
            'successes': 0,
            'failures': 0,
            'attempts_per_second': 0.0,
            'success_rate': 0.0
        }
        
        while time.time() - start_time < duration:
            result = await self._execute_exploit_test(exploit_code, target_url, 1)
            
            benchmark['total_attempts'] += 1
            
            if result['success']:
                benchmark['successes'] += 1
            else:
                benchmark['failures'] += 1
            
            # Small delay to avoid overwhelming target
            await asyncio.sleep(0.1)
        
        actual_duration = time.time() - start_time
        benchmark['attempts_per_second'] = benchmark['total_attempts'] / actual_duration
        benchmark['success_rate'] = benchmark['successes'] / benchmark['total_attempts'] if benchmark['total_attempts'] > 0 else 0.0
        
        log.info(f"[ExploitTester] Benchmark complete: {benchmark['attempts_per_second']:.2f} attempts/s, "
                f"{benchmark['success_rate']:.1%} success rate")
        
        return benchmark
    
    async def generate_test_report(self) -> str:
        """Generate test report"""
        
        report = []
        report.append("=" * 80)
        report.append("EXPLOIT TEST REPORT")
        report.append("=" * 80)
        report.append("")
        
        report.append(f"Total Tests: {len(self.test_results)}")
        report.append(f"Total Successes: {self.success_count}")
        report.append(f"Total Failures: {self.failure_count}")
        report.append("")
        
        if self.test_results:
            avg_success_rate = sum(r['success_rate'] for r in self.test_results) / len(self.test_results)
            avg_reliability = sum(r['reliability_score'] for r in self.test_results) / len(self.test_results)
            
            report.append(f"Average Success Rate: {avg_success_rate:.1%}")
            report.append(f"Average Reliability: {avg_reliability:.2f}")
            report.append("")
            
            report.append("Individual Test Results:")
            report.append("-" * 80)
            
            for i, result in enumerate(self.test_results, 1):
                report.append(f"\nTest {i}: {result['target']}")
                report.append(f"  Success Rate: {result['success_rate']:.1%}")
                report.append(f"  Reliability: {result['reliability_score']:.2f}")
                report.append(f"  Avg Response Time: {result['avg_response_time']:.3f}s")
                
                if result['errors']:
                    report.append(f"  Errors: {len(result['errors'])}")
        
        report.append("")
        report.append("=" * 80)
        
        return '\n'.join(report)


if __name__ == '__main__':
    async def test():
        tester = ExploitTester()
        
        # Test single target
        exploit = "mock_exploit_code"
        target = "http://test.com"
        
        results = await tester.test_exploit(exploit, target, iterations=10)
        
        print("Test Results:")
        print(f"  Success Rate: {results['success_rate']:.1%}")
        print(f"  Reliability Score: {results['reliability_score']:.2f}")
        print(f"  Avg Response Time: {results['avg_response_time']:.3f}s")
        
        # Test multiple targets
        targets = [
            "http://test1.com",
            "http://test2.com",
            "http://test3.com"
        ]
        
        multi_results = await tester.test_multi_target(exploit, targets, iterations_per_target=5)
        
        print(f"\nMulti-Target Results:")
        print(f"  Successful Targets: {multi_results['successful_targets']}/{multi_results['total_targets']}")
        print(f"  Overall Success Rate: {multi_results['overall_success_rate']:.1%}")
        
        # Generate report
        report = await tester.generate_test_report()
        print(f"\n{report}")
    
    asyncio.run(test())

