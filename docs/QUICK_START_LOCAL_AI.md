# Quick Start: ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Local AI (Ollama) ‡∏Å‡∏±‡∏ö dLNk Attack Platform
## ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ - ‡∏ü‡∏£‡∏µ 100%

---

## üìã ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

‚úÖ Ollama ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß  
‚úÖ Models ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
- mixtral:latest (26GB)
- llama3:8b-instruct-fp16 (16GB)
- llama3:latest (4.7GB)
- codellama:latest (3.8GB)
- mistral:latest (4.4GB)

---

## üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ollama

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
curl http://localhost:11434/api/tags

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö model
ollama run mixtral:latest "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ"
```

### Step 2: Clone Repository

```bash
git clone https://github.com/donlasahachat1-sys/manus.git
cd manus
```

### Step 3: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Environment

```bash
# Copy environment file
cp .env.production .env

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç .env
nano .env
```

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô**:
```bash
# Database
POSTGRES_PASSWORD=your_strong_password

# Redis
REDIS_PASSWORD=your_redis_password

# C2
C2_ENCRYPTION_KEY=your_32_character_encryption_key_here

# Grafana
GRAFANA_ADMIN_PASSWORD=your_grafana_password

# LLM (‡πÉ‡∏ä‡πâ Ollama - ‡∏ü‡∏£‡∏µ)
LLM_PROVIDER=ollama  # default ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
```

**‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤**:
- ‚ùå OPENAI_API_KEY (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ)
- ‚ùå SLACK_WEBHOOK_URL (optional)
- ‚ùå DISCORD_WEBHOOK_URL (optional)
- ‚ùå TELEGRAM_BOT_TOKEN (optional)

### Step 4: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Ollama Model

‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÑ‡∏ü‡∏•‡πå `llm_config.py`:

```python
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ï‡∏≤‡∏° RAM ‡∏ó‡∏µ‡πà‡∏°‡∏µ
OLLAMA_MODEL = "mixtral:latest"  # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ RAM 32GB+)
# ‡∏´‡∏£‡∏∑‡∏≠
OLLAMA_MODEL = "llama3:latest"  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ RAM ‡∏ô‡πâ‡∏≠‡∏¢ (8-16GB)
```

### Step 5: Start Services

```bash
# Build ‡πÅ‡∏•‡∏∞ start
docker-compose -f docker-compose.production.yml up -d

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
docker-compose -f docker-compose.production.yml ps
```

### Step 6: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```bash
# API Health
curl localhost:8000/health

# Ollama Health
curl http://localhost:11434/api/tags

# Prometheus
curl http://localhost:9090/-/healthy
```

---

## üéØ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á Services

| Service | URL | Description |
|---------|-----|-------------|
| Frontend | http://localhost | Web UI |
| API | localhost:8000 | REST API |
| API Docs | localhost:8000/docs | Swagger UI |
| Grafana | http://localhost:3000 | Monitoring |
| Prometheus | http://localhost:9090 | Metrics |
| Kibana | http://localhost:5601 | Logs |

### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö AI Integration

```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô API
curl -X POST localhost:8000/api/ai/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡πà‡∏≠‡∏á‡πÇ‡∏´‡∏ß‡πà SQL Injection"
  }'
```

### ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Attack

```bash
# ‡∏™‡πà‡∏á attack request
curl -X POST localhost:8000/api/attack \
  -H "Content-Type: application/json" \
  -d '{
    "target": "localhost:8000",
    "attack_type": "web_application"
  }'
```

---

## üîß ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Model

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç llm_config.py

```python
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
OLLAMA_MODEL = "llama3:latest"  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô model ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÉ‡∏ä‡πâ Environment Variable

```bash
# ‡πÉ‡∏ô .env
OLLAMA_MODEL=llama3:latest
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: Runtime (‡∏ú‡πà‡∏≤‡∏ô API)

```python
from llm_config import get_llm_config

# Override model
config = get_llm_config()
config['model'] = 'llama3:latest'
```

---

## üìä ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ‡∏ï‡∏≤‡∏° Use Case

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Vulnerability Analysis (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)
```python
OLLAMA_MODEL = "mixtral:latest"  # Best accuracy
# ‡∏´‡∏£‡∏∑‡∏≠
OLLAMA_MODEL = "llama3:8b-instruct-fp16"  # High quality
```

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Code Generation
```python
OLLAMA_MODEL = "codellama:latest"  # Specialized for code
```

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fast Response (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
```python
OLLAMA_MODEL = "llama3:latest"  # Fastest
# ‡∏´‡∏£‡∏∑‡∏≠
OLLAMA_MODEL = "mistral:latest"  # Good balance
```

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Development/Testing
```python
OLLAMA_MODEL = "mistral:latest"  # ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM
```

---

## üéì ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### 1. Vulnerability Scanning with AI

```python
from core.ai_integration import AIOrchestrator

# Initialize with Ollama
orchestrator = AIOrchestrator()

# Analyze target
result = await orchestrator.analyze_target("https://localhost:8000")
print(result)
```

### 2. AI-Powered Exploit Generation

```python
from advanced_agents.exploit_generator import ExploitGenerator

generator = ExploitGenerator()

# Generate exploit
exploit = await generator.generate_exploit(
    vulnerability_type="SQL Injection",
    target_info={"url": "https://localhost:8000/login"}
)
```

### 3. Reinforcement Learning Attack

```python
from core.rl_attack_agent import RLAttackOrchestrator

orchestrator = RLAttackOrchestrator()

# Train (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)
await orchestrator.train(episodes=100)

# Attack with RL
result = await orchestrator.attack_with_rl("https://localhost:8000")
```

---

## üêõ Troubleshooting

### Ollama ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö process
ps aux | grep ollama

# Restart Ollama
docker-compose -f docker-compose.production.yml restart ollama

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs
docker-compose -f docker-compose.production.yml logs ollama
```

### Model ‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ model
ollama list

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î model ‡πÉ‡∏´‡∏°‡πà
ollama pull mixtral:latest

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö model
ollama run mixtral:latest "test"
```

### API ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á

```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs
docker-compose -f docker-compose.production.yml logs api

# Restart API
docker-compose -f docker-compose.production.yml restart api
```

### Out of Memory

```bash
# ‡πÉ‡∏ä‡πâ model ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á
OLLAMA_MODEL=llama3:latest  # 4.7GB

# ‡∏´‡∏£‡∏∑‡∏≠
OLLAMA_MODEL=mistral:latest  # 4.4GB

# Restart services
docker-compose -f docker-compose.production.yml restart
```

---

## üí° Tips & Tricks

### 1. ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM

```bash
# ‡∏õ‡∏¥‡∏î services ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
docker-compose -f docker-compose.production.yml stop grafana kibana

# ‡πÉ‡∏ä‡πâ model ‡πÄ‡∏•‡πá‡∏Å
OLLAMA_MODEL=mistral:latest
```

### 2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß

```bash
# ‡πÉ‡∏ä‡πâ model ‡πÄ‡∏•‡πá‡∏Å
OLLAMA_MODEL=llama3:latest

# ‡πÄ‡∏û‡∏¥‡πà‡∏° CPU cores ‡πÉ‡∏´‡πâ Ollama
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç docker-compose.production.yml:
services:
  ollama:
    deploy:
      resources:
        limits:
          cpus: '8'  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 4
```

### 3. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

```bash
# ‡πÉ‡∏ä‡πâ model ‡πÉ‡∏´‡∏ç‡πà
OLLAMA_MODEL=mixtral:latest

# ‡∏õ‡∏£‡∏±‡∏ö temperature
# ‡πÉ‡∏ô llm_config.py:
TEMPERATURE = 0.3  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.7 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
```

### 4. Warm-up Model

```bash
# ‡πÇ‡∏´‡∏•‡∏î model ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
ollama run mixtral:latest "warm up"

# ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡πà‡∏≤‡∏ô API
curl http://localhost:11434/api/generate \
  -d '{"model": "mixtral:latest", "prompt": "warm up"}'
```

---

## üìà Performance Benchmarks

### Model Comparison (‡∏ö‡∏ô Server 32GB RAM)

| Model | Load Time | Response Time | Quality | RAM Usage |
|-------|-----------|---------------|---------|-----------|
| mixtral:latest | 30s | 5-10s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 26GB |
| llama3:8b-instruct-fp16 | 20s | 3-7s | ‚≠ê‚≠ê‚≠ê‚≠ê | 16GB |
| llama3:latest | 10s | 2-5s | ‚≠ê‚≠ê‚≠ê | 5GB |
| codellama:latest | 10s | 2-5s | ‚≠ê‚≠ê‚≠ê (code) | 4GB |
| mistral:latest | 10s | 2-5s | ‚≠ê‚≠ê‚≠ê | 5GB |

---

## üéØ Next Steps

1. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö Attack Scenarios**:
```bash
# ‡∏î‡∏π examples
ls examples/

# ‡∏£‡∏±‡∏ô example
python examples/sql_injection_test.py
```

2. **‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Notifications** (Optional):
```bash
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç .env
SLACK_WEBHOOK_URL=your_webhook
DISCORD_WEBHOOK_URL=your_webhook
```

3. **‡∏î‡∏π Monitoring**:
- Grafana: http://localhost:3000
- Prometheus: http://localhost:9090
- Kibana: http://localhost:5601

4. **‡∏≠‡πà‡∏≤‡∏ô Documentation ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°**:
- `docs/FREE_TOOLS_SETUP.md` - ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ü‡∏£‡∏µ
- `docs/DEVELOPMENT_SUMMARY.md` - ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤
- `docs/CRITICAL_POINTS_ANALYSIS.md` - ‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

---

## ‚úÖ Checklist

- [x] Ollama ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
- [x] Models ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- [x] Docker Compose start ‡πÅ‡∏•‡πâ‡∏ß
- [x] API ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á
- [x] Frontend ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ
- [ ] ‡∏ó‡∏î‡∏™‡∏≠‡∏ö attack (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ)
- [ ] ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ notifications (optional)
- [ ] ‡∏î‡∏π monitoring dashboards

---

## üéâ ‡∏™‡∏£‡∏∏‡∏õ

‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô dLNk Attack Platform ‡πÅ‡∏•‡πâ‡∏ß!

‚úÖ ‡πÉ‡∏ä‡πâ Local AI (Ollama) - ‡∏ü‡∏£‡∏µ 100%  
‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô API  
‚úÖ ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á  
‚úÖ Privacy & Security  

**Happy Hacking! üöÄ**

---

**‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤**:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs: `docker-compose logs -f`
2. ‡∏≠‡πà‡∏≤‡∏ô Troubleshooting ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô
3. ‡∏î‡∏π documentation ‡πÉ‡∏ô `docs/`

