"""
Vulnerability Classifier using Machine Learning
Enhanced version with multiple models and comprehensive feature extraction
"""

import asyncio
import numpy as np
import pickle
import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import logging

log = logging.getLogger(__name__)

try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import classification_report, confusion_matrix
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    log.warning("[VulnClassifier] scikit-learn not available. Install: pip install scikit-learn")


class VulnerabilityClassifier:
    """
    ML-based Vulnerability Classifier
    
    Features:
    - Multi-class classification of vulnerability types
    - Support for multiple ML models (Random Forest, Gradient Boosting)
    - Feature extraction from code patterns, network traffic, and system logs
    - Model persistence (save/load)
    - Confidence scoring
    - Cross-validation
    
    Vulnerability Types:
    - SQL Injection
    - Cross-Site Scripting (XSS)
    - Command Injection
    - Path Traversal
    - SSRF (Server-Side Request Forgery)
    - Authentication Bypass
    - Deserialization
    - Buffer Overflow
    - Use-After-Free
    - Zero-Day (Unknown)
    """
    
    def __init__(self, model_type: str = 'random_forest'):
        self.model_type = model_type
        self.model = None
        self.label_encoder = None
        self.scaler = None
        self.is_trained = False
        self.feature_names = []
        self.vulnerability_types = [
            'sql_injection',
            'xss_reflected',
            'xss_stored',
            'xss_dom',
            'command_injection',
            'path_traversal',
            'ssrf',
            'auth_bypass',
            'deserialization',
            'buffer_overflow',
            'use_after_free',
            'format_string',
            'race_condition',
            'xxe',
            'csrf',
            'zero_day'
        ]
        
        if SKLEARN_AVAILABLE:
            self._initialize_model()
    
    def _initialize_model(self):
        """Initialize ML model and preprocessing tools"""
        if self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'
            )
        elif self.model_type == 'gradient_boosting':
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42
            )
        else:
            log.warning(f"[VulnClassifier] Unknown model type: {self.model_type}, using Random Forest")
            self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        
        self.label_encoder = LabelEncoder()
        self.scaler = StandardScaler()
    
    async def train(
        self,
        features: List[List[float]],
        labels: List[str],
        feature_names: Optional[List[str]] = None,
        validation_split: float = 0.2
    ) -> Dict:
        """
        Train the classifier with cross-validation
        
        Args:
            features: Feature vectors
            labels: Vulnerability type labels
            feature_names: Names of features (for interpretability)
            validation_split: Fraction of data for validation
        
        Returns:
            Training results with metrics
        """
        if not SKLEARN_AVAILABLE:
            log.warning("[VulnClassifier] Training skipped - scikit-learn not available")
            return {'success': False, 'error': 'scikit-learn not available'}
        
        if len(features) == 0 or len(labels) == 0:
            log.error("[VulnClassifier] No training data provided")
            return {'success': False, 'error': 'No training data'}
        
        log.info(f"[VulnClassifier] Training on {len(features)} samples")
        log.info(f"[VulnClassifier] Model type: {self.model_type}")
        
        try:
            # Store feature names
            if feature_names:
                self.feature_names = feature_names
            
            # Convert to numpy arrays
            X = np.array(features)
            y = np.array(labels)
            
            # Encode labels
            y_encoded = self.label_encoder.fit_transform(y)
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            # Split data
            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y_encoded,
                test_size=validation_split,
                random_state=42,
                stratify=y_encoded
            )
            
            # Train model
            log.info("[VulnClassifier] Training model...")
            self.model.fit(X_train, y_train)
            
            # Evaluate on validation set
            val_score = self.model.score(X_val, y_val)
            
            # Cross-validation
            log.info("[VulnClassifier] Running cross-validation...")
            cv_scores = cross_val_score(self.model, X_scaled, y_encoded, cv=5)
            
            # Predictions for detailed metrics
            y_pred = self.model.predict(X_val)
            
            # Classification report
            report = classification_report(
                y_val, y_pred,
                target_names=self.label_encoder.classes_,
                output_dict=True,
                zero_division=0
            )
            
            self.is_trained = True
            
            results = {
                'success': True,
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'validation_accuracy': float(val_score),
                'cv_scores': cv_scores.tolist(),
                'cv_mean': float(cv_scores.mean()),
                'cv_std': float(cv_scores.std()),
                'classification_report': report,
                'feature_count': X.shape[1]
            }
            
            log.info(f"[VulnClassifier] Training complete")
            log.info(f"  - Validation Accuracy: {val_score:.4f}")
            log.info(f"  - CV Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
            
            return results
            
        except Exception as e:
            log.error(f"[VulnClassifier] Training failed: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
    
    async def predict(
        self,
        features: List[float],
        return_probabilities: bool = True
    ) -> Dict:
        """
        Predict vulnerability type from features
        
        Args:
            features: Feature vector
            return_probabilities: Whether to return probability distribution
        
        Returns:
            Prediction results with confidence
        """
        if not SKLEARN_AVAILABLE or not self.is_trained:
            return await self._fallback_predict(features)
        
        try:
            # Convert to numpy array and scale
            features_array = np.array([features])
            features_scaled = self.scaler.transform(features_array)
            
            # Predict
            prediction = self.model.predict(features_scaled)[0]
            probabilities = self.model.predict_proba(features_scaled)[0]
            
            # Decode label
            vuln_type = self.label_encoder.inverse_transform([prediction])[0]
            confidence = float(max(probabilities))
            
            result = {
                'vulnerability_type': vuln_type,
                'confidence': confidence,
                'prediction_quality': self._assess_prediction_quality(confidence)
            }
            
            if return_probabilities:
                result['all_probabilities'] = {
                    self.label_encoder.inverse_transform([i])[0]: float(prob)
                    for i, prob in enumerate(probabilities)
                }
                
                # Top 3 predictions
                top_3_idx = np.argsort(probabilities)[-3:][::-1]
                result['top_3_predictions'] = [
                    {
                        'type': self.label_encoder.inverse_transform([idx])[0],
                        'confidence': float(probabilities[idx])
                    }
                    for idx in top_3_idx
                ]
            
            # Feature importance (if available)
            if hasattr(self.model, 'feature_importances_') and self.feature_names:
                importances = self.model.feature_importances_
                top_features_idx = np.argsort(importances)[-5:][::-1]
                result['important_features'] = [
                    {
                        'name': self.feature_names[idx],
                        'importance': float(importances[idx])
                    }
                    for idx in top_features_idx
                    if idx < len(self.feature_names)
                ]
            
            return result
            
        except Exception as e:
            log.error(f"[VulnClassifier] Prediction failed: {e}")
            return await self._fallback_predict(features)
    
    def _assess_prediction_quality(self, confidence: float) -> str:
        """Assess the quality of prediction based on confidence"""
        if confidence >= 0.9:
            return 'HIGH'
        elif confidence >= 0.7:
            return 'MEDIUM'
        elif confidence >= 0.5:
            return 'LOW'
        else:
            return 'VERY_LOW'
    
    async def predict_batch(
        self,
        features_batch: List[List[float]]
    ) -> List[Dict]:
        """
        Predict vulnerability types for multiple samples
        
        Args:
            features_batch: List of feature vectors
        
        Returns:
            List of prediction results
        """
        results = []
        for features in features_batch:
            result = await self.predict(features, return_probabilities=False)
            results.append(result)
        return results
    
    async def _fallback_predict(self, features: List[float]) -> Dict:
        """Fallback rule-based prediction when ML is not available"""
        log.debug("[VulnClassifier] Using fallback prediction")
        
        # Simple rule-based heuristics
        if not features or len(features) == 0:
            return {
                'vulnerability_type': 'unknown',
                'confidence': 0.0,
                'prediction_quality': 'VERY_LOW',
                'message': 'No features provided'
            }
        
        # Use feature patterns to guess vulnerability type
        max_feature_idx = features.index(max(features))
        vuln_type = self.vulnerability_types[max_feature_idx % len(self.vulnerability_types)]
        
        # Calculate pseudo-confidence based on feature values
        feature_sum = sum(abs(f) for f in features)
        max_feature = max(abs(f) for f in features)
        confidence = min(max_feature / (feature_sum + 1e-6), 1.0) if feature_sum > 0 else 0.3
        
        return {
            'vulnerability_type': vuln_type,
            'confidence': float(confidence),
            'prediction_quality': self._assess_prediction_quality(confidence),
            'fallback': True,
            'message': 'Rule-based prediction (ML not available)'
        }
    
    async def save_model(self, model_path: str) -> bool:
        """
        Save trained model to disk
        
        Args:
            model_path: Path to save model
        
        Returns:
            Success status
        """
        if not self.is_trained:
            log.error("[VulnClassifier] Cannot save untrained model")
            return False
        
        try:
            model_data = {
                'model': self.model,
                'label_encoder': self.label_encoder,
                'scaler': self.scaler,
                'feature_names': self.feature_names,
                'model_type': self.model_type,
                'vulnerability_types': self.vulnerability_types
            }
            
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            log.info(f"[VulnClassifier] Model saved to {model_path}")
            return True
            
        except Exception as e:
            log.error(f"[VulnClassifier] Failed to save model: {e}")
            return False
    
    async def load_model(self, model_path: str) -> bool:
        """
        Load trained model from disk
        
        Args:
            model_path: Path to model file
        
        Returns:
            Success status
        """
        if not Path(model_path).exists():
            log.error(f"[VulnClassifier] Model file not found: {model_path}")
            return False
        
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.model = model_data['model']
            self.label_encoder = model_data['label_encoder']
            self.scaler = model_data['scaler']
            self.feature_names = model_data.get('feature_names', [])
            self.model_type = model_data.get('model_type', 'random_forest')
            self.vulnerability_types = model_data.get('vulnerability_types', self.vulnerability_types)
            self.is_trained = True
            
            log.info(f"[VulnClassifier] Model loaded from {model_path}")
            return True
            
        except Exception as e:
            log.error(f"[VulnClassifier] Failed to load model: {e}")
            return False
    
    def get_model_info(self) -> Dict:
        """Get information about the current model"""
        return {
            'model_type': self.model_type,
            'is_trained': self.is_trained,
            'sklearn_available': SKLEARN_AVAILABLE,
            'feature_count': len(self.feature_names),
            'vulnerability_types': self.vulnerability_types,
            'classes': self.label_encoder.classes_.tolist() if self.label_encoder and hasattr(self.label_encoder, 'classes_') else []
        }


# Feature extraction utilities
class VulnerabilityFeatureExtractor:
    """Extract features from various sources for vulnerability classification"""
    
    @staticmethod
    async def extract_from_code(code: str, language: str = 'python') -> List[float]:
        """Extract features from source code"""
        features = []
        
        # SQL-related keywords
        sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'DROP', 'execute', 'query']
        features.append(sum(1 for kw in sql_keywords if kw.lower() in code.lower()))
        
        # XSS-related patterns
        xss_patterns = ['innerHTML', 'document.write', 'eval', '<script', 'javascript:']
        features.append(sum(1 for pat in xss_patterns if pat.lower() in code.lower()))
        
        # Command execution
        cmd_patterns = ['system', 'exec', 'shell_exec', 'popen', 'subprocess']
        features.append(sum(1 for pat in cmd_patterns if pat.lower() in code.lower()))
        
        # File operations
        file_patterns = ['open', 'read', 'write', 'file_get_contents', 'include']
        features.append(sum(1 for pat in file_patterns if pat.lower() in code.lower()))
        
        # User input sources
        input_patterns = ['$_GET', '$_POST', 'request.', 'input(', 'argv']
        features.append(sum(1 for pat in input_patterns if pat in code))
        
        # Deserialization
        deser_patterns = ['unserialize', 'pickle.loads', 'yaml.load', 'json.loads']
        features.append(sum(1 for pat in deser_patterns if pat in code))
        
        # Authentication
        auth_patterns = ['password', 'login', 'auth', 'session', 'token']
        features.append(sum(1 for pat in auth_patterns if pat.lower() in code.lower()))
        
        # Network operations
        net_patterns = ['http', 'curl', 'requests', 'fetch', 'ajax']
        features.append(sum(1 for pat in net_patterns if pat.lower() in code.lower()))
        
        # Code length and complexity
        features.append(len(code))
        features.append(code.count('\n'))
        
        return features
    
    @staticmethod
    async def extract_from_http_request(request_data: Dict) -> List[float]:
        """Extract features from HTTP request"""
        features = []
        
        url = request_data.get('url', '')
        headers = request_data.get('headers', {})
        body = request_data.get('body', '')
        method = request_data.get('method', 'GET')
        
        # SQL injection indicators
        sql_chars = ["'", '"', '--', ';', 'UNION', 'SELECT']
        features.append(sum(1 for char in sql_chars if char in url or char in body))
        
        # XSS indicators
        xss_chars = ['<script', 'javascript:', 'onerror=', 'onload=']
        features.append(sum(1 for char in xss_chars if char in url or char in body))
        
        # Command injection indicators
        cmd_chars = ['|', '&', ';', '`', '$', '$(']
        features.append(sum(1 for char in cmd_chars if char in url or char in body))
        
        # Path traversal indicators
        path_chars = ['../', '..\\', '%2e%2e']
        features.append(sum(1 for char in path_chars if char in url))
        
        # SSRF indicators
        ssrf_patterns = ['localhost', '127.0.0.1', '0.0.0.0', 'file://', 'gopher://']
        features.append(sum(1 for pat in ssrf_patterns if pat in url or pat in body))
        
        # Request characteristics
        features.append(len(url))
        features.append(len(body))
        features.append(1 if method == 'POST' else 0)
        features.append(len(headers))
        
        return features


if __name__ == '__main__':
    async def test():
        """Test the classifier"""
        classifier = VulnerabilityClassifier(model_type='random_forest')
        
        print(f"[*] Model info: {classifier.get_model_info()}")
        
        # Mock training data
        features = [
            [10, 0, 0, 0, 5, 0, 0, 0, 100, 10],  # SQL injection
            [0, 10, 0, 0, 5, 0, 0, 0, 100, 10],  # XSS
            [0, 0, 10, 0, 5, 0, 0, 0, 100, 10],  # Command injection
        ]
        labels = ['sql_injection', 'xss_reflected', 'command_injection']
        
        # Train
        results = await classifier.train(features, labels)
        print(f"\n[+] Training results: {results}")
        
        # Predict
        test_features = [8, 1, 0, 0, 4, 0, 0, 0, 90, 8]
        prediction = await classifier.predict(test_features)
        print(f"\n[+] Prediction: {prediction}")
    
    asyncio.run(test())

