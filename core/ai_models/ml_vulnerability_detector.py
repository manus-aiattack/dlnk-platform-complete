"""
ML-Based Vulnerability Detector
Uses machine learning for vulnerability pattern recognition
"""

import asyncio
import numpy as np
import re
import json
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import logging

log = logging.getLogger(__name__)


class MLVulnerabilityDetector:
    """
    Machine Learning-based Vulnerability Detector
    
    Features:
    - Pattern-based vulnerability detection
    - Feature extraction from HTTP responses
    - Anomaly detection
    - Confidence scoring
    """
    
    def __init__(self, model_path: str = None):
        self.model_path = model_path or "/tmp/dlnk_ml_model.json"
        self.vulnerability_patterns = self._load_patterns()
        self.feature_weights = self._initialize_weights()
        
    def _load_patterns(self) -> Dict:
        """Load vulnerability patterns"""
        
        return {
            'sql_injection': {
                'patterns': [
                    r"error in your SQL syntax",
                    r"mysql_fetch",
                    r"ORA-\d+",
                    r"PostgreSQL.*ERROR",
                    r"SQLite.*error",
                    r"ODBC.*Driver",
                    r"Microsoft SQL Server",
                    r"Unclosed quotation mark",
                ],
                'severity': 'CRITICAL',
                'cvss_score': 9.8,
                'cwe_id': 'CWE-89'
            },
            
            'xss': {
                'patterns': [
                    r"<script[^>]*>.*</script>",
                    r"javascript:",
                    r"onerror\s*=",
                    r"onload\s*=",
                    r"eval\s*\(",
                ],
                'severity': 'HIGH',
                'cvss_score': 7.3,
                'cwe_id': 'CWE-79'
            },
            
            'rce': {
                'patterns': [
                    r"sh:\s+.*:\s+command not found",
                    r"bash:\s+.*:\s+command not found",
                    r"system\(\)",
                    r"exec\(\)",
                    r"eval\(\)",
                    r"passthru\(",
                    r"shell_exec\(",
                ],
                'severity': 'CRITICAL',
                'cvss_score': 9.9,
                'cwe_id': 'CWE-78'
            },
            
            'lfi': {
                'patterns': [
                    r"root:x:0:0:",
                    r"\[boot loader\]",
                    r"<?php",
                    r"Warning.*failed to open stream",
                ],
                'severity': 'HIGH',
                'cvss_score': 7.5,
                'cwe_id': 'CWE-22'
            },
            
            'ssrf': {
                'patterns': [
                    r"Connection refused",
                    r"Internal Server Error",
                    r"localhost",
                    r"127\.0\.0\.1",
                    r"169\.254\.169\.254",
                ],
                'severity': 'HIGH',
                'cvss_score': 8.6,
                'cwe_id': 'CWE-918'
            },
            
            'xxe': {
                'patterns': [
                    r"<!DOCTYPE",
                    r"<!ENTITY",
                    r"SYSTEM\s+['\"]file://",
                ],
                'severity': 'HIGH',
                'cvss_score': 7.5,
                'cwe_id': 'CWE-611'
            },
            
            'deserialization': {
                'patterns': [
                    r"unserialize\(",
                    r"pickle\.loads",
                    r"yaml\.load",
                    r"ObjectInputStream",
                ],
                'severity': 'CRITICAL',
                'cvss_score': 9.8,
                'cwe_id': 'CWE-502'
            },
            
            'auth_bypass': {
                'patterns': [
                    r"admin",
                    r"administrator",
                    r"root",
                    r"logged in as",
                    r"authentication.*bypass",
                ],
                'severity': 'CRITICAL',
                'cvss_score': 9.1,
                'cwe_id': 'CWE-287'
            },
            
            'idor': {
                'patterns': [
                    r"user_id=\d+",
                    r"id=\d+",
                    r"account_id=\d+",
                ],
                'severity': 'HIGH',
                'cvss_score': 7.1,
                'cwe_id': 'CWE-639'
            },
            
            'csrf': {
                'patterns': [
                    r"<form[^>]*>",
                    r"action\s*=",
                ],
                'severity': 'MEDIUM',
                'cvss_score': 6.5,
                'cwe_id': 'CWE-352'
            },
        }
    
    def _initialize_weights(self) -> Dict:
        """Initialize feature weights for ML model"""
        
        return {
            'pattern_match': 0.4,
            'response_anomaly': 0.3,
            'status_code': 0.15,
            'response_time': 0.10,
            'response_size': 0.05
        }
    
    async def detect_vulnerabilities(
        self,
        response_data: str,
        status_code: int = 200,
        response_time: float = 0.0,
        response_size: int = 0,
        url: str = "",
        payload: str = ""
    ) -> List[Dict]:
        """
        Detect vulnerabilities using ML-based analysis
        
        Args:
            response_data: HTTP response body
            status_code: HTTP status code
            response_time: Response time in seconds
            response_size: Response size in bytes
            url: Request URL
            payload: Attack payload used
        
        Returns:
            List of detected vulnerabilities with confidence scores
        """
        log.info("[MLDetector] Analyzing response for vulnerabilities")
        
        vulnerabilities = []
        
        # Extract features
        features = self._extract_features(
            response_data, status_code, response_time, response_size
        )
        
        # Pattern matching
        for vuln_type, vuln_info in self.vulnerability_patterns.items():
            confidence = await self._calculate_confidence(
                vuln_type, vuln_info, response_data, features
            )
            
            if confidence > 0.5:  # Threshold
                vulnerabilities.append({
                    'type': vuln_type,
                    'severity': vuln_info['severity'],
                    'cvss_score': vuln_info['cvss_score'],
                    'cwe_id': vuln_info['cwe_id'],
                    'confidence': confidence,
                    'location': url,
                    'payload': payload,
                    'evidence': self._extract_evidence(response_data, vuln_type),
                    'description': self._generate_description(vuln_type, confidence),
                    'remediation': self._generate_remediation(vuln_type)
                })
        
        # Sort by confidence
        vulnerabilities.sort(key=lambda x: x['confidence'], reverse=True)
        
        log.info(f"[MLDetector] Detected {len(vulnerabilities)} vulnerabilities")
        
        return vulnerabilities
    
    def _extract_features(
        self,
        response_data: str,
        status_code: int,
        response_time: float,
        response_size: int
    ) -> Dict:
        """Extract features from response for ML analysis"""
        
        features = {
            'status_code': status_code,
            'response_time': response_time,
            'response_size': response_size,
            'has_error': self._has_error_indicators(response_data),
            'has_stack_trace': self._has_stack_trace(response_data),
            'has_sql_keywords': self._has_sql_keywords(response_data),
            'has_script_tags': self._has_script_tags(response_data),
            'has_file_paths': self._has_file_paths(response_data),
            'entropy': self._calculate_entropy(response_data),
            'special_char_ratio': self._calculate_special_char_ratio(response_data)
        }
        
        return features
    
    async def _calculate_confidence(
        self,
        vuln_type: str,
        vuln_info: Dict,
        response_data: str,
        features: Dict
    ) -> float:
        """Calculate confidence score using weighted features"""
        
        confidence = 0.0
        
        # Pattern matching score
        pattern_score = 0.0
        for pattern in vuln_info['patterns']:
            if re.search(pattern, response_data, re.IGNORECASE):
                pattern_score = 1.0
                break
        
        confidence += pattern_score * self.feature_weights['pattern_match']
        
        # Anomaly score
        anomaly_score = self._calculate_anomaly_score(features)
        confidence += anomaly_score * self.feature_weights['response_anomaly']
        
        # Status code score
        status_score = self._calculate_status_score(features['status_code'])
        confidence += status_score * self.feature_weights['status_code']
        
        # Response time score
        time_score = self._calculate_time_score(features['response_time'])
        confidence += time_score * self.feature_weights['response_time']
        
        # Response size score
        size_score = self._calculate_size_score(features['response_size'])
        confidence += size_score * self.feature_weights['response_size']
        
        return min(confidence, 1.0)
    
    def _calculate_anomaly_score(self, features: Dict) -> float:
        """Calculate anomaly score based on features"""
        
        score = 0.0
        
        if features['has_error']:
            score += 0.3
        if features['has_stack_trace']:
            score += 0.3
        if features['has_sql_keywords']:
            score += 0.2
        if features['has_script_tags']:
            score += 0.1
        if features['has_file_paths']:
            score += 0.1
        
        return min(score, 1.0)
    
    def _calculate_status_score(self, status_code: int) -> float:
        """Calculate score based on status code"""
        
        if status_code >= 500:
            return 0.8
        elif status_code >= 400:
            return 0.5
        elif status_code >= 300:
            return 0.3
        else:
            return 0.1
    
    def _calculate_time_score(self, response_time: float) -> float:
        """Calculate score based on response time"""
        
        # Longer response time might indicate time-based attacks
        if response_time > 5.0:
            return 0.9
        elif response_time > 3.0:
            return 0.7
        elif response_time > 1.0:
            return 0.5
        else:
            return 0.1
    
    def _calculate_size_score(self, response_size: int) -> float:
        """Calculate score based on response size"""
        
        # Unusually large or small responses might be suspicious
        if response_size > 100000 or response_size < 10:
            return 0.7
        elif response_size > 50000 or response_size < 50:
            return 0.5
        else:
            return 0.1
    
    def _has_error_indicators(self, response_data: str) -> bool:
        """Check for error indicators"""
        error_keywords = ['error', 'exception', 'warning', 'fatal', 'failed']
        return any(keyword in response_data.lower() for keyword in error_keywords)
    
    def _has_stack_trace(self, response_data: str) -> bool:
        """Check for stack trace"""
        stack_trace_patterns = [
            r'at\s+\w+\.\w+\(',
            r'File\s+"[^"]+",\s+line\s+\d+',
            r'Traceback\s+\(most recent call last\)',
        ]
        return any(re.search(pattern, response_data) for pattern in stack_trace_patterns)
    
    def _has_sql_keywords(self, response_data: str) -> bool:
        """Check for SQL keywords"""
        sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'FROM', 'WHERE', 'UNION']
        return any(keyword in response_data.upper() for keyword in sql_keywords)
    
    def _has_script_tags(self, response_data: str) -> bool:
        """Check for script tags"""
        return bool(re.search(r'<script[^>]*>', response_data, re.IGNORECASE))
    
    def _has_file_paths(self, response_data: str) -> bool:
        """Check for file paths"""
        path_patterns = [
            r'/etc/passwd',
            r'C:\\Windows',
            r'/var/www',
            r'/usr/bin',
        ]
        return any(re.search(pattern, response_data, re.IGNORECASE) for pattern in path_patterns)
    
    def _calculate_entropy(self, data: str) -> float:
        """Calculate Shannon entropy"""
        if not data:
            return 0.0
        
        entropy = 0.0
        for char in set(data):
            prob = data.count(char) / len(data)
            entropy -= prob * np.log2(prob)
        
        return entropy
    
    def _calculate_special_char_ratio(self, data: str) -> float:
        """Calculate ratio of special characters"""
        if not data:
            return 0.0
        
        special_chars = sum(1 for char in data if not char.isalnum() and not char.isspace())
        return special_chars / len(data)
    
    def _extract_evidence(self, response_data: str, vuln_type: str) -> str:
        """Extract evidence for vulnerability"""
        
        # Extract relevant portion of response
        max_length = 500
        
        # Find the most relevant part
        patterns = self.vulnerability_patterns[vuln_type]['patterns']
        for pattern in patterns:
            match = re.search(pattern, response_data, re.IGNORECASE)
            if match:
                start = max(0, match.start() - 100)
                end = min(len(response_data), match.end() + 100)
                return response_data[start:end]
        
        # Return first part if no match found
        return response_data[:max_length]
    
    def _generate_description(self, vuln_type: str, confidence: float) -> str:
        """Generate vulnerability description"""
        
        descriptions = {
            'sql_injection': f"SQL Injection vulnerability detected with {confidence:.1%} confidence. The application appears to be vulnerable to SQL injection attacks.",
            'xss': f"Cross-Site Scripting (XSS) vulnerability detected with {confidence:.1%} confidence. User input is not properly sanitized.",
            'rce': f"Remote Code Execution (RCE) vulnerability detected with {confidence:.1%} confidence. The application may allow arbitrary code execution.",
            'lfi': f"Local File Inclusion (LFI) vulnerability detected with {confidence:.1%} confidence. The application may allow reading arbitrary files.",
            'ssrf': f"Server-Side Request Forgery (SSRF) vulnerability detected with {confidence:.1%} confidence. The application may allow making requests to internal resources.",
            'xxe': f"XML External Entity (XXE) vulnerability detected with {confidence:.1%} confidence. The application may be vulnerable to XXE attacks.",
            'deserialization': f"Insecure Deserialization vulnerability detected with {confidence:.1%} confidence. The application may allow arbitrary object injection.",
            'auth_bypass': f"Authentication Bypass vulnerability detected with {confidence:.1%} confidence. The application's authentication mechanism may be bypassed.",
            'idor': f"Insecure Direct Object Reference (IDOR) vulnerability detected with {confidence:.1%} confidence. The application may allow unauthorized access to objects.",
            'csrf': f"Cross-Site Request Forgery (CSRF) vulnerability detected with {confidence:.1%} confidence. The application may not properly validate requests.",
        }
        
        return descriptions.get(vuln_type, f"Vulnerability of type {vuln_type} detected with {confidence:.1%} confidence.")
    
    def _generate_remediation(self, vuln_type: str) -> str:
        """Generate remediation advice"""
        
        remediations = {
            'sql_injection': "Use parameterized queries or prepared statements. Never concatenate user input directly into SQL queries.",
            'xss': "Sanitize and encode all user input before displaying it. Use Content Security Policy (CSP) headers.",
            'rce': "Avoid using dangerous functions like eval(), exec(), system(). Validate and sanitize all user input.",
            'lfi': "Validate and sanitize file paths. Use whitelisting for allowed files. Avoid using user input in file operations.",
            'ssrf': "Validate and sanitize URLs. Use whitelisting for allowed domains. Implement network segmentation.",
            'xxe': "Disable external entity processing in XML parsers. Use secure XML parsing libraries.",
            'deserialization': "Avoid deserializing untrusted data. Use safe serialization formats like JSON. Implement integrity checks.",
            'auth_bypass': "Implement proper authentication and authorization checks. Use secure session management.",
            'idor': "Implement proper access control checks. Use indirect object references. Validate user permissions.",
            'csrf': "Implement CSRF tokens. Use SameSite cookie attribute. Validate Origin and Referer headers.",
        }
        
        return remediations.get(vuln_type, "Consult security best practices for remediation.")
    
    async def train_model(self, training_data: List[Dict]):
        """Train the ML model with new data"""
        
        log.info(f"[MLDetector] Training model with {len(training_data)} samples")
        
        # Update feature weights based on training data
        # This is a simplified version - in production, use proper ML algorithms
        
        for sample in training_data:
            vuln_type = sample.get('vulnerability_type')
            success = sample.get('success', False)
            
            if success and vuln_type in self.vulnerability_patterns:
                # Increase confidence for successful detections
                # This is a placeholder - implement actual ML training
                pass
        
        # Save updated model
        self._save_model()
        
        log.info("[MLDetector] Model training completed")
    
    def _save_model(self):
        """Save model to disk"""
        
        model_data = {
            'feature_weights': self.feature_weights,
            'vulnerability_patterns': self.vulnerability_patterns
        }
        
        try:
            with open(self.model_path, 'w') as f:
                json.dump(model_data, f, indent=2)
            log.info(f"[MLDetector] Model saved to {self.model_path}")
        except Exception as e:
            log.error(f"[MLDetector] Failed to save model: {e}")


if __name__ == '__main__':
    async def test():
        detector = MLVulnerabilityDetector()
        
        # Test SQL injection detection
        response = "mysql_fetch_array() error in your SQL syntax near '1=1'"
        vulns = await detector.detect_vulnerabilities(
            response_data=response,
            status_code=500,
            response_time=0.5,
            response_size=len(response),
            url='http://test.com/api/users',
            payload="' OR 1=1--"
        )
        
        print(f"Detected {len(vulns)} vulnerabilities:")
        for vuln in vulns:
            print(f"  - {vuln['type']}: {vuln['severity']} (confidence: {vuln['confidence']:.2%})")
            print(f"    CWE: {vuln['cwe_id']}, CVSS: {vuln['cvss_score']}")
            print(f"    {vuln['description']}")
    
    asyncio.run(test())

