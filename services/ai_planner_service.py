"""
AI Planner Service with Local LLM Integration
Provides intelligent attack planning and strategy generation using a local LLM
"""

import asyncio
import json
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx

from core.logger import log
from core.context_manager import ContextManager
from core.data_models import Strategy, AttackPhase


class AttackObjective(str, Enum):
    BACKDOOR = "backdoor"
    COMMAND_EXECUTION = "command_execution"
    DATA_EXFILTRATION = "data_exfiltration"
    PRIVILEGE_ESCALATION = "privilege_escalation"
    LATERAL_MOVEMENT = "lateral_movement"
    PERSISTENCE = "persistence"


@dataclass
class TargetIntelligence:
    """Target intelligence data structure"""
    target_url: str
    ip_address: Optional[str] = None
    open_ports: List[int] = None
    services: Dict[str, str] = None
    technologies: List[str] = None
    vulnerabilities: List[Dict[str, Any]] = None
    waf_detected: bool = False
    os_type: Optional[str] = None


@dataclass
class AttackPlan:
    """Attack plan generated by AI"""
    objective: AttackObjective
    confidence: float
    estimated_time: int
    risk_level: str
    steps: List[Dict[str, Any]]
    fallback_strategies: List[Dict[str, Any]]
    evasion_techniques: List[str]


class LocalLLMEngine:
    """Local LLM Engine for attack planning"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        
    async def initialize(self):
        """Initialize the LLM model"""
        try:
            log.info(f"Loading LLM model from {self.model_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1
            )
            
            log.success(f"LLM model loaded successfully on {self.device}")
            
        except Exception as e:
            log.error(f"Failed to initialize LLM: {e}")
            raise
    
    async def generate_attack_plan(
        self,
        objective: AttackObjective,
        target_intel: TargetIntelligence,
        threat_intel: Dict[str, Any],
        available_agents: List[str]
    ) -> AttackPlan:
        """Generate attack plan using LLM"""
        
        # Construct prompt for LLM
        prompt = self._construct_planning_prompt(
            objective, target_intel, threat_intel, available_agents
        )
        
        try:
            # Generate response from LLM
            response = self.pipeline(
                prompt,
                max_new_tokens=2048,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                return_full_text=False
            )
            
            # Parse LLM response into structured attack plan
            plan_text = response[0]['generated_text']
            attack_plan = self._parse_llm_response(plan_text, objective)
            
            log.info(f"Generated attack plan for objective: {objective}")
            return attack_plan
            
        except Exception as e:
            log.error(f"Failed to generate attack plan: {e}")
            raise
    
    def _construct_planning_prompt(
        self,
        objective: AttackObjective,
        target_intel: TargetIntelligence,
        threat_intel: Dict[str, Any],
        available_agents: List[str]
    ) -> str:
        """Construct prompt for LLM"""
        
        prompt = f"""You are an expert offensive security AI planner. Your task is to create a detailed attack plan.

**Objective:** {objective.value}

**Target Intelligence:**
- URL: {target_intel.target_url}
- IP: {target_intel.ip_address}
- Open Ports: {target_intel.open_ports}
- Services: {json.dumps(target_intel.services, indent=2)}
- Technologies: {target_intel.technologies}
- OS: {target_intel.os_type}
- WAF Detected: {target_intel.waf_detected}

**Known Vulnerabilities:**
{json.dumps(target_intel.vulnerabilities, indent=2)}

**Available Agents:**
{', '.join(available_agents)}

**Threat Intelligence:**
{json.dumps(threat_intel, indent=2)}

**Instructions:**
Create a detailed, step-by-step attack plan to achieve the objective. For each step, specify:
1. Agent to use
2. Specific action/directive
3. Expected outcome
4. Fallback options
5. Evasion techniques if needed

Output the plan in JSON format with the following structure:
{{
  "confidence": 0.0-1.0,
  "estimated_time": seconds,
  "risk_level": "low/medium/high",
  "steps": [
    {{
      "step_number": 1,
      "agent": "AgentName",
      "action": "specific action",
      "expected_outcome": "what to achieve",
      "parameters": {{}},
      "fallback": "alternative approach"
    }}
  ],
  "fallback_strategies": [],
  "evasion_techniques": []
}}

Generate the attack plan:"""
        
        return prompt
    
    def _parse_llm_response(self, response_text: str, objective: AttackObjective) -> AttackPlan:
        """Parse LLM response into AttackPlan"""
        try:
            # Extract JSON from response
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("No JSON found in LLM response")
            
            json_str = response_text[start_idx:end_idx]
            plan_data = json.loads(json_str)
            
            return AttackPlan(
                objective=objective,
                confidence=plan_data.get('confidence', 0.5),
                estimated_time=plan_data.get('estimated_time', 3600),
                risk_level=plan_data.get('risk_level', 'medium'),
                steps=plan_data.get('steps', []),
                fallback_strategies=plan_data.get('fallback_strategies', []),
                evasion_techniques=plan_data.get('evasion_techniques', [])
            )
            
        except Exception as e:
            log.error(f"Failed to parse LLM response: {e}")
            # Return a basic fallback plan
            return AttackPlan(
                objective=objective,
                confidence=0.3,
                estimated_time=3600,
                risk_level='high',
                steps=[],
                fallback_strategies=[],
                evasion_techniques=[]
            )


class ReinforcementLearningEngine:
    """Reinforcement Learning Engine for continuous improvement"""
    
    def __init__(self, model_path: str, context_manager: ContextManager):
        self.model_path = model_path
        self.context_manager = context_manager
        self.q_table = {}
        self.learning_rate = 0.1
        self.discount_factor = 0.95
        self.epsilon = 0.1
        
    async def initialize(self):
        """Initialize RL engine"""
        try:
            # Load existing Q-table if available
            if os.path.exists(f"{self.model_path}/q_table.json"):
                with open(f"{self.model_path}/q_table.json", 'r') as f:
                    self.q_table = json.load(f)
            log.success("RL Engine initialized")
        except Exception as e:
            log.error(f"Failed to initialize RL engine: {e}")
    
    async def record_attack_outcome(
        self,
        attack_plan: AttackPlan,
        success: bool,
        time_taken: int,
        detected: bool
    ):
        """Record attack outcome for learning"""
        try:
            # Calculate reward
            reward = self._calculate_reward(success, time_taken, detected, attack_plan.estimated_time)
            
            # Update Q-table
            state_key = self._get_state_key(attack_plan)
            if state_key not in self.q_table:
                self.q_table[state_key] = 0.0
            
            self.q_table[state_key] = self.q_table[state_key] + \
                self.learning_rate * (reward - self.q_table[state_key])
            
            # Save updated Q-table
            await self._save_q_table()
            
            log.info(f"Recorded attack outcome: success={success}, reward={reward}")
            
        except Exception as e:
            log.error(f"Failed to record attack outcome: {e}")
    
    def _calculate_reward(self, success: bool, time_taken: int, detected: bool, estimated_time: int) -> float:
        """Calculate reward for RL"""
        reward = 0.0
        
        if success:
            reward += 100.0
            
            # Bonus for efficiency
            if time_taken < estimated_time:
                reward += 20.0
        else:
            reward -= 50.0
        
        # Penalty for detection
        if detected:
            reward -= 30.0
        else:
            reward += 10.0
        
        return reward
    
    def _get_state_key(self, attack_plan: AttackPlan) -> str:
        """Generate state key for Q-table"""
        return f"{attack_plan.objective}_{attack_plan.risk_level}_{len(attack_plan.steps)}"
    
    async def _save_q_table(self):
        """Save Q-table to disk"""
        try:
            os.makedirs(self.model_path, exist_ok=True)
            with open(f"{self.model_path}/q_table.json", 'w') as f:
                json.dump(self.q_table, f, indent=2)
        except Exception as e:
            log.error(f"Failed to save Q-table: {e}")


# FastAPI Application
app = FastAPI(title="AI Planner Service")

# Global instances
llm_engine: Optional[LocalLLMEngine] = None
rl_engine: Optional[ReinforcementLearningEngine] = None
context_manager: Optional[ContextManager] = None


class PlanRequest(BaseModel):
    objective: AttackObjective
    target_intel: Dict[str, Any]
    threat_intel: Dict[str, Any]
    available_agents: List[str]


class OutcomeRequest(BaseModel):
    attack_plan: Dict[str, Any]
    success: bool
    time_taken: int
    detected: bool


@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    global llm_engine, rl_engine, context_manager
    
    try:
        # Initialize Context Manager
        context_manager = ContextManager()
        await context_manager.setup()
        
        # Initialize LLM Engine
        model_path = os.getenv("LLM_MODEL_PATH", "/models/llm/mistral-7b-instruct")
        llm_engine = LocalLLMEngine(model_path)
        await llm_engine.initialize()
        
        # Initialize RL Engine
        rl_model_path = os.getenv("RL_MODEL_PATH", "/models/rl/attack_optimizer")
        rl_engine = ReinforcementLearningEngine(rl_model_path, context_manager)
        await rl_engine.initialize()
        
        log.success("AI Planner Service started successfully")
        
    except Exception as e:
        log.error(f"Failed to start AI Planner Service: {e}")
        raise


@app.post("/plan")
async def create_attack_plan(request: PlanRequest):
    """Create attack plan endpoint"""
    try:
        target_intel = TargetIntelligence(**request.target_intel)
        
        attack_plan = await llm_engine.generate_attack_plan(
            objective=request.objective,
            target_intel=target_intel,
            threat_intel=request.threat_intel,
            available_agents=request.available_agents
        )
        
        return asdict(attack_plan)
        
    except Exception as e:
        log.error(f"Failed to create attack plan: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/feedback")
async def record_feedback(request: OutcomeRequest):
    """Record attack outcome for learning"""
    try:
        attack_plan = AttackPlan(**request.attack_plan)
        
        await rl_engine.record_attack_outcome(
            attack_plan=attack_plan,
            success=request.success,
            time_taken=request.time_taken,
            detected=request.detected
        )
        
        return {"status": "success", "message": "Feedback recorded"}
        
    except Exception as e:
        log.error(f"Failed to record feedback: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "llm_loaded": llm_engine is not None}


@app.get("/ready")
async def readiness_check():
    """Readiness check endpoint"""
    if llm_engine is None or rl_engine is None:
        raise HTTPException(status_code=503, detail="Service not ready")
    return {"status": "ready"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)

