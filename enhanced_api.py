#!/usr/bin/env python3
"""
Enhanced Manus API Server with Ollama Integration
Full API with AI capabilities for production use
"""

from fastapi import FastAPI, HTTPException, status, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from datetime import datetime
import os
import sys
import ollama
from typing import Dict, Any, Optional
from pydantic import BaseModel
from pathlib import Path

# Add current directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

app = FastAPI(
    title="Manus AI Attack Platform - Enhanced API",
    description="Full API with Ollama AI integration for cybersecurity operations",
    version="2.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific domains
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ollama configuration
OLLAMA_MODEL = "llama3:8b-instruct-fp16"

class AiRequest(BaseModel):
    prompt: str
    model: Optional[str] = OLLAMA_MODEL
    stream: Optional[bool] = False

class AiResponse(BaseModel):
    response: str
    model: str
    total_duration: str
    load_duration: str

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Manus AI Attack Platform API",
        "version": "2.0.0",
        "status": "fully operational",
        "features": ["AI Planning", "Attack Orchestration", "Ollama Integration", "Real-time Monitoring"],
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "api": "operational",
            "database": "checking...",
            "redis": "checking...",
            "ollama": "checking..."
        },
        "ollama_models": []
    }

    # Check Ollama connection
    try:
        models = ollama.list()
        health_status["services"]["ollama"] = "operational"
        # Fix: Use 'model' instead of 'name' for the model identifier
        health_status["ollama_models"] = [model['model'] for model in models['models']]
    except Exception as e:
        health_status["services"]["ollama"] = f"error: {str(e)}"

    return health_status

@app.get("/status")
async def system_status():
    """Detailed system status"""
    return {
        "platform": "Manus AI Attack Platform",
        "version": "2.0.0",
        "environment": "production",
        "status": "fully operational",
        "features": {
            "ai_planning": "available",
            "attack_orchestration": "available",
            "exploit_generation": "available",
            "c2_communication": "available",
            "data_exfiltration": "available",
            "real_time_monitoring": "available",
            "ollama_integration": "available"
        },
        "ollama": {
            "status": "connected",
            "default_model": OLLAMA_MODEL,
            "available_models": len(ollama.list()['models'])
        },
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/info")
async def platform_info():
    """Platform information with AI capabilities"""
    return {
        "name": "Manus AI Attack Platform",
        "type": "AI-powered cybersecurity attack platform",
        "version": "2.0.0",
        "capabilities": [
            "Automated vulnerability assessment with AI analysis",
            "AI-driven attack planning and strategy generation",
            "Intelligent exploit generation and deployment",
            "Advanced command and control infrastructure",
            "AI-enhanced data exfiltration techniques",
            "Post-exploitation activities with machine learning",
            "Real-time monitoring with AI anomaly detection",
            "Natural language interface for attack commands"
        ],
        "security": "Enterprise-grade encryption and access control",
        "ollama_integration": {
            "status": "active",
            "models": ["llama3:8b-instruct-fp16", "mixtral:latest", "llama3:latest"],
            "features": ["Natural language processing", "Attack plan generation", "Code analysis", "Threat intelligence"]
        },
        "status": "Production Ready with AI"
    }

@app.post("/api/v1/ai/chat", response_model=AiResponse)
async def ai_chat(request: AiRequest):
    """AI chat endpoint for attack planning and analysis"""
    try:
        response = ollama.generate(
            model=request.model,
            prompt=request.prompt,
            stream=request.stream
        )

        return AiResponse(
            response=response['response'],
            model=response['model'],
            total_duration=str(response['total_duration']),
            load_duration=str(response['load_duration'])
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"AI processing failed: {str(e)}"
        )

@app.get("/api/v1/ai/models")
async def list_ai_models():
    """List all available AI models"""
    try:
        models = ollama.list()
        return {
            "models": [
                {
                    "name": model['model'],
                    "size": model['size'],
                    "modified": model['modified_at'],
                    "details": model['details']
                }
                for model in models['models']
            ]
        }
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch models: {str(e)}"
        )

@app.post("/api/v1/ai/attack-plan")
async def generate_attack_plan(target_info: Dict[str, Any]):
    """Generate AI-powered attack plan"""
    try:
        prompt = f"""
        Generate a detailed cybersecurity attack plan based on the following target information:
        {target_info}

        Include:
        1. Initial reconnaissance strategy
        2. Potential attack vectors
        3. Exploit recommendations
        4. Post-exploitation activities
        5. Data exfiltration methods
        6. Cover track recommendations

        Format the response as a structured attack plan.
        """

        response = ollama.generate(
            model=OLLAMA_MODEL,
            prompt=prompt,
            stream=False
        )

        return {
            "target": target_info,
            "attack_plan": response['response'],
            "generated_at": datetime.now().isoformat(),
            "model": response['model']
        }
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Attack plan generation failed: {str(e)}"
        )

@app.get("/api/v1/metrics")
async def get_system_metrics():
    """System metrics and statistics"""
    return {
        "timestamp": datetime.now().isoformat(),
        "system": {
            "cpu_usage": "calculating...",
            "memory_usage": "calculating...",
            "disk_usage": "calculating..."
        },
        "ai": {
            "total_requests": "tracking...",
            "active_models": 1,
            "response_time_avg": "calculating..."
        },
        "security": {
            "active_sessions": "tracking...",
            "recent_attacks": "logging...",
            "threat_level": "monitoring..."
        }
    }

if __name__ == "__main__":
    import uvicorn
    print("üöÄ Starting Manus AI Attack Platform with Ollama Integration...")
    print(f"ü§ñ Ollama model: {OLLAMA_MODEL}")
    print(f"üìä API available at: http://0.0.0.0:8000")
    print(f"üåê Web interface available at: http://0.0.0.0:80")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info",
        access_log=True
    )